1. Requirements.txt
2. Readme update

## Introduction
Museum Data Pipeline
Introduction
The Museum Data Pipeline is a project that processes collection data from museums and creates a summarized dataset containing key information about objects in the museum. The pipeline is designed to extract relevant information from raw museum data, such as the type of object, creation date, title of the object, and name of the museum. This summarized dataset serves as a valuable resource for researchers, historians, and curators to gain insights into the museum's collection without the need to sift through large amounts of raw data.

The Museum Data Pipeline offers a scalable and automated solution for processing and summarizing collection data from museums, making it easier to extract meaningful information and generate insights. The pipeline can be easily integrated into existing museum data management workflows, enabling museums to efficiently and effectively process their collection data.

### Why is this project useful?
The Museum Data Pipeline offers several key benefits:
1. **Efficient Data Processing**: The pipeline automates the extraction and summarization of key information from raw museum data, saving time and effort for museum staff. It enables museums to process large amounts of data quickly and accurately, reducing the manual labor required for data processing tasks.
2. **Insight Generation**: The summarized dataset generated by the pipeline provides valuable insights into the museum's collection, such as the types of objects, creation dates, and titles of objects. This information can be used by researchers, historians, and curators to gain a better understanding of the museum's collection and support their research and curation efforts.
3. **Data Accessibility**: The summarized dataset created by the pipeline offers a user-friendly format for accessing and analyzing museum collection data. This makes it easier for museum staff and researchers to explore and analyze the data, even without specialized technical skills or knowledge in data processing.
4. **Integration into Existing Workflows**: The Museum Data Pipeline is designed to be easily integrated into existing museum data management workflows. It can be customized to meet the specific needs of different museums and seamlessly integrated into their existing data processing pipelines, making it a practical and efficient solution for managing collection data.
5. **Scalability**: The pipeline is scalable, allowing museums to process large volumes of data as their collections grow over time. It can handle different types of data sources and formats, making it adaptable to the evolving needs of museums.

In summary, the Museum Data Pipeline offers an efficient, scalable, and user-friendly solution for processing and summarizing collection data from museums. It provides valuable insights into museum collections and supports research, curation, and data management efforts.

## The project
The goal of the project is to get an idea of the general offering of the most popular museums.
The goal was to setup a generalized data pipeline, which would be able to process data from different museums and be able to show summarised details from them.

![alt text](architecture.png "Architecture")

The project utilizes Google Cloud and its offered features, including:
- **Cloud Storage**: used to store raw museum data
- **BigQuery**: data warehouse solution, which enables processing the raw data and stores processed information about the museum items
- **Dataproc Serverless**: Google's Spark solution, which runs on demand, without the need to manage the Spark cluster itself. It is used to transform raw data into the desired form
- **Data Studio**: although not necessarily part of Google Cloud, but it is a Google product. It serves as the visualization solution for our data

## How to
1. Make sure Python is updated with requirements.txt
2. Prepare the infrastructure
   1. Download credentials for Google Cloud
   2. Run the setup.sh script
   3. Run Terraform / update the properties in the var file
3. Upload files manually / with the provided script to Google Cloud storage
4. Wait / Check in Google BigQuery that the items are picked up ???
5. Run the Spark script to transform the data (script must be created for unsupported museum types)
6. View the data (Is there a way to save the setup for Dat Studio?)


https://medium.com/cts-technologies/running-pyspark-jobs-on-google-cloud-using-serverless-dataproc-f16cef5ec6b9
https://github.com/kevenpinto/spark-serverless-repo-example/blob/master/Makefile

Make sure that python is installed
1. Create project by hand
2. Make sure billing is enabled
3. Install gcloud and authenticate https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/provider_reference#running-terraform-on-your-workstation
so you can run the Terraform commands
4. Run Terraform
  1. Update the var files (need a common for the project and the region) with project id
  2. Run with init
5. Upload the files
  1. Download or copy github
    - https://github.com/tategallery/collection
    - https://github.com/cooperhewitt/collection
    - https://github.com/artsmia/collection
  2. Run the shell script to make the JSON files new line delimited JSON files
  3. Upload the files with the provided python script
6. Run Terraform with bq vars
7. Run the spark jobs with the provided commands
8. Create the Studio

## TODO 

- Fix tate collection spark script to process the record type from bigquery
- Figure out how to run pyspark jobs on dataproc serverless
- 


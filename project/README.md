1. Requirements.txt
     
2. Readme update

## Introduction

Museum Data Pipeline
Introduction
The Museum Data Pipeline is a project that processes collection data from museums and creates a summarized dataset
containing key information about objects in the museum. The pipeline is designed to extract relevant information from
raw museum data, such as the type of object, creation date, title of the object, and name of the museum. This summarized
dataset serves as a valuable resource for researchers, historians, and curators to gain insights into the museum's
collection without the need to sift through large amounts of raw data.

The Museum Data Pipeline offers a scalable and automated solution for processing and summarizing collection data from
museums, making it easier to extract meaningful information and generate insights. The pipeline can be easily integrated
into existing museum data management workflows, enabling museums to efficiently and effectively process their collection
data.

### Why is this project useful?

The Museum Data Pipeline offers several key benefits:

1. **Efficient Data Processing**: The pipeline automates the extraction and summarization of key information from raw
   museum data, saving time and effort for museum staff. It enables museums to process large amounts of data quickly and
   accurately, reducing the manual labor required for data processing tasks.
2. **Insight Generation**: The summarized dataset generated by the pipeline provides valuable insights into the museum's
   collection, such as the types of objects, creation dates, and titles of objects. This information can be used by
   researchers, historians, and curators to gain a better understanding of the museum's collection and support their
   research and curation efforts.
3. **Data Accessibility**: The summarized dataset created by the pipeline offers a user-friendly format for accessing
   and analyzing museum collection data. This makes it easier for museum staff and researchers to explore and analyze
   the data, even without specialized technical skills or knowledge in data processing.
4. **Integration into Existing Workflows**: The Museum Data Pipeline is designed to be easily integrated into existing
   museum data management workflows. It can be customized to meet the specific needs of different museums and seamlessly
   integrated into their existing data processing pipelines, making it a practical and efficient solution for managing
   collection data.
5. **Scalability**: The pipeline is scalable, allowing museums to process large volumes of data as their collections
   grow over time. It can handle different types of data sources and formats, making it adaptable to the evolving needs
   of museums.

In summary, the Museum Data Pipeline offers an efficient, scalable, and user-friendly solution for processing and
summarizing collection data from museums. It provides valuable insights into museum collections and supports research,
curation, and data management efforts.

## The project

The goal of the project is to get an idea of the general offering of the most popular museums.
The goal was to setup a generalized data pipeline, which would be able to process data from different museums and be
able to show summarised details from them.

![alt text](architecture.png "Architecture")

The project utilizes Google Cloud and its offered features, including:

- **Cloud Storage**: used to store raw museum data
- **BigQuery**: data warehouse solution, which enables processing the raw data and stores processed information about
  the museum items
- **Dataproc Serverless**: Google's Spark solution, which runs on demand, without the need to manage the Spark cluster
  itself. It is used to transform raw data into the desired form
- **Data Studio**: although not necessarily part of Google Cloud, but it is a Google product. It serves as the
  visualization solution for our data

## How to

1. Make sure Python is updated with requirements.txt
2. Prepare the infrastructure
    1. Download credentials for Google Cloud
    2. Run the setup.sh script
    3. Run Terraform / update the properties in the var file
3. Upload files manually / with the provided script to Google Cloud storage
4. Wait / Check in Google BigQuery that the items are picked up ???
5. Run the Spark script to transform the data (script must be created for unsupported museum types)
6. View the data (Is there a way to save the setup for Dat Studio?)

https://medium.com/cts-technologies/running-pyspark-jobs-on-google-cloud-using-serverless-dataproc-f16cef5ec6b9

https://github.com/kevenpinto/spark-serverless-repo-example/blob/master/Makefile

https://cloud.google.com/dataproc-serverless/docs/guides/bigquery-connector-spark-example


Make sure that python is installed

1. Create project by hand
2. Make sure billing is enabled
3. Install gcloud and
   authenticate https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/provider_reference#running-terraform-on-your-workstation
   so you can run the Terraform commands
4. Run Terraform
    1. Update the var files (need a common for the project and the region) with project id
    2. Run in terraform directory
       - `terraform init`
       - `terraform apply -var-file="./vars/init.tfvars"`
5. Upload the files
    1. Download or copy github or skip this test and use test_data, which includes a smaller subset of data
        - https://github.com/tategallery/collection
        - https://github.com/cooperhewitt/collection
        - https://github.com/artsmia/collection
    2. Run the shell script to make the JSON files new line delimited JSON files
        - `./util/convert_json_to_single_line.sh ./test_data/cooper_hevit/raw ./test_data/cooper_hevit/one_line`
        - `./util/convert_json_to_single_line.sh ./test_data/mia/raw ./test_data/mia/one_line`
        - `./util/convert_json_to_single_line.sh ./test_data/tate/raw ./test_data/tate/one_line`
    3. Upload the files with the provided python script
        - `python3 ./upload_json.py project-id cooper-hevit-bucket-name ./test_data/cooper_hevit/one_line`
        - `python3 ./upload_json.py project-id tate-bucket-name ./test_data/tate/one_line`
        - `python3 ./upload_json.py project-id mia-bucket-name ./test_data/mia/one_line`
6. Run Terraform with bq vars
    - In terraform directory `terraform apply -var-file="./vars/bq.tfvars"`
7. Run the spark jobs with the provided commands in the Spark directory
    - `./run_spark.sh cooper_hevit_summary_transform.py google-region spark_temp`
    - `./run_spark.sh mia_summary_transform.py region spark_temp`
    - `./run_spark.sh tate_summary_transform.py region spark_temp`
8. Create the Report with Studio

## TODO

- Store the terraform state in a bucket - terraform is updated, only the bucket creation is missing
- Make the spark temp bucket randomized name
- Maybe add logging to the upload scripts to know where we are
- Update readme to use makefile
- Add auto apply to terraform
- Update spark scripts to process JSON
- Create custom VPC with private network access enabled, might need to update BQ too and the spark submit must be also updated
https://stackoverflow.com/questions/44189120/processing-json-strings-in-a-spark-dataframe-column
https://sparkbyexamples.com/pyspark/pyspark-json-functions-with-examples/ - get json object
